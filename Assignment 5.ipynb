{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f158fc63-8e30-4030-a140-5ed46259c67a",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb5f367a-e4b1-467e-8e53-9d6aaf702536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94f6342e-3c97-40bd-9b1c-56b96eb9ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_labels = ['spam', 'ham', 'spam', 'spam', 'spam',\n",
    "               'ham', 'ham', 'spam', 'ham', 'spam',\n",
    "               'spam', 'ham', 'ham', 'ham', 'spam',\n",
    "               'ham', 'ham', 'spam', 'spam', 'ham']\n",
    "\n",
    "predicted_labels = ['spam', 'spam', 'spam', 'ham', 'spam',\n",
    "                    'spam', 'ham', 'ham', 'spam', 'spam',\n",
    "                    'ham', 'ham', 'spam', 'ham', 'ham',\n",
    "                    'ham', 'spam', 'ham', 'spam', 'spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf3f5b10-a868-4fe3-8c17-4359bfb54d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "actual = []\n",
    "for i in actual_labels:\n",
    "    if i == 'spam':\n",
    "        actual.append(1)\n",
    "    else:\n",
    "        actual.append(0)\n",
    "print(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7239e5c9-6865-46a3-bfaf-8d6e13608d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "for i in predicted_labels:\n",
    "    if i == 'spam':\n",
    "        predicted.append(1)\n",
    "    else:\n",
    "        predicted.append(0)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "096be961-2211-4dc9-a2f6-46ee3a253218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "924a6083-0282-47d2-a674-8d09401fde07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score : 0.45454545454545453\n",
      "Recall Score : 0.5\n",
      "Accuracy Score : 0.45\n",
      "F1 Score : 0.47619047619047616\n"
     ]
    }
   ],
   "source": [
    "print('Precision Score :', precision_score(actual, predicted))\n",
    "print('Recall Score :',recall_score(actual, predicted))\n",
    "print('Accuracy Score :',accuracy_score(actual, predicted))\n",
    "print('F1 Score :',f1_score(actual,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a471baa9-8b89-4490-b141-0e84d46e7933",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "323d8440-e08b-4495-ac89-f9e9d144383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "020535a2-f250-4748-a369-db50f1247701",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = 'The idea that masses of autonomous vehicles could someday shuttle passengers through city streets and down US highways raises some big questions'\n",
    "sent2 = 'Will a computer ever be able to match the safety and intuition of a human driver?'\n",
    "sent3 = 'Will we need to rethink how streets are designed?'\n",
    "sent4 = 'Does society need this technology in the first place?'\n",
    "\n",
    "corpus = [sent1 , sent2, sent3, sent4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbbc0dab-4751-4f62-be1f-4a818fd4107f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAG OF WORDS\n",
      "[[0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1\n",
      "  0 1 1 0 1 0 1 1 0 0]\n",
      " [1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 1 0 0 1 0 0 0 1]\n",
      " [0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1\n",
      "  0 0 0 0 0 1 0 0 1 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0\n",
      "  1 0 1 1 0 0 0 0 0 0]]\n",
      "-------------------------------------------------\n",
      "FEATURES\n",
      "['able' 'and' 'are' 'autonomous' 'be' 'big' 'city' 'computer' 'could'\n",
      " 'designed' 'does' 'down' 'driver' 'ever' 'first' 'highways' 'how' 'human'\n",
      " 'idea' 'in' 'intuition' 'masses' 'match' 'need' 'of' 'passengers' 'place'\n",
      " 'questions' 'raises' 'rethink' 'safety' 'shuttle' 'society' 'some'\n",
      " 'someday' 'streets' 'technology' 'that' 'the' 'this' 'through' 'to' 'us'\n",
      " 'vehicles' 'we' 'will']\n",
      "-------------------------------------------------\n",
      "VOCABULARY\n",
      "{'the': 38, 'idea': 18, 'that': 37, 'masses': 21, 'of': 24, 'autonomous': 3, 'vehicles': 43, 'could': 8, 'someday': 34, 'shuttle': 31, 'passengers': 25, 'through': 40, 'city': 6, 'streets': 35, 'and': 1, 'down': 11, 'us': 42, 'highways': 15, 'raises': 28, 'some': 33, 'big': 5, 'questions': 27, 'will': 45, 'computer': 7, 'ever': 13, 'be': 4, 'able': 0, 'to': 41, 'match': 22, 'safety': 30, 'intuition': 20, 'human': 17, 'driver': 12, 'we': 44, 'need': 23, 'rethink': 29, 'how': 16, 'are': 2, 'designed': 9, 'does': 10, 'society': 32, 'this': 39, 'technology': 36, 'in': 19, 'first': 14, 'place': 26}\n"
     ]
    }
   ],
   "source": [
    "def bag_of_words_cv(corpus):\n",
    "    vectorizer = CountVectorizer()\n",
    "    matrix = vectorizer.fit_transform(corpus)\n",
    "    print('BAG OF WORDS')\n",
    "    print(matrix.toarray())\n",
    "    print('-------------------------------------------------')\n",
    "    print('FEATURES')\n",
    "    print(vectorizer.get_feature_names_out())\n",
    "    print('-------------------------------------------------')\n",
    "    print('VOCABULARY')\n",
    "    print(vectorizer.vocabulary_)\n",
    "\n",
    "bag_of_words_cv(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04fbfcab-b0de-4f07-963b-6d5801d2f053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAG OF WORDS\n",
      "[[0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1\n",
      "  0 0 1 0 0 1 1 0 0 0]\n",
      " [1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 1 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 1 0 0 1 0 1]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0\n",
      "  0 1 0 0 0 0 0 0 0 0]]\n",
      "-------------------------------------------------\n",
      "FEATURES\n",
      "['able to match' 'and down us' 'and intuition of'\n",
      " 'autonomous vehicles could' 'be able to' 'city streets and'\n",
      " 'computer ever be' 'could someday shuttle' 'does society need'\n",
      " 'down us highways' 'ever be able' 'highways raises some'\n",
      " 'how streets are' 'idea that masses' 'in the first' 'intuition of human'\n",
      " 'masses of autonomous' 'match the safety' 'need this technology'\n",
      " 'need to rethink' 'of autonomous vehicles' 'of human driver'\n",
      " 'passengers through city' 'raises some big' 'rethink how streets'\n",
      " 'safety and intuition' 'shuttle passengers through' 'society need this'\n",
      " 'some big questions' 'someday shuttle passengers' 'streets and down'\n",
      " 'streets are designed' 'technology in the' 'that masses of'\n",
      " 'the first place' 'the idea that' 'the safety and' 'this technology in'\n",
      " 'through city streets' 'to match the' 'to rethink how'\n",
      " 'us highways raises' 'vehicles could someday' 'we need to'\n",
      " 'will computer ever' 'will we need']\n",
      "-------------------------------------------------\n",
      "VOCABULARY\n",
      "{'the idea that': 35, 'idea that masses': 13, 'that masses of': 33, 'masses of autonomous': 16, 'of autonomous vehicles': 20, 'autonomous vehicles could': 3, 'vehicles could someday': 42, 'could someday shuttle': 7, 'someday shuttle passengers': 29, 'shuttle passengers through': 26, 'passengers through city': 22, 'through city streets': 38, 'city streets and': 5, 'streets and down': 30, 'and down us': 1, 'down us highways': 9, 'us highways raises': 41, 'highways raises some': 11, 'raises some big': 23, 'some big questions': 28, 'will computer ever': 44, 'computer ever be': 6, 'ever be able': 10, 'be able to': 4, 'able to match': 0, 'to match the': 39, 'match the safety': 17, 'the safety and': 36, 'safety and intuition': 25, 'and intuition of': 2, 'intuition of human': 15, 'of human driver': 21, 'will we need': 45, 'we need to': 43, 'need to rethink': 19, 'to rethink how': 40, 'rethink how streets': 24, 'how streets are': 12, 'streets are designed': 31, 'does society need': 8, 'society need this': 27, 'need this technology': 18, 'this technology in': 37, 'technology in the': 32, 'in the first': 14, 'the first place': 34}\n"
     ]
    }
   ],
   "source": [
    "def bag_of_words_3gram(corpus):\n",
    "    vectorizer = CountVectorizer(ngram_range=(3,3))\n",
    "    matrix = vectorizer.fit_transform(corpus)\n",
    "    print('BAG OF WORDS')\n",
    "    print(matrix.toarray())\n",
    "    print('-------------------------------------------------')\n",
    "    print('FEATURES')\n",
    "    print(vectorizer.get_feature_names_out())\n",
    "    print('-------------------------------------------------')\n",
    "    print('VOCABULARY')\n",
    "    print(vectorizer.vocabulary_)\n",
    "    \n",
    "bag_of_words_3gram(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c37b549d-f001-4d7b-a14b-ec941736f26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAG OF WORDS\n",
      "[[0.         0.2236068  0.         0.2236068  0.         0.2236068\n",
      "  0.         0.2236068  0.         0.2236068  0.         0.2236068\n",
      "  0.         0.2236068  0.         0.         0.2236068  0.\n",
      "  0.         0.         0.2236068  0.         0.2236068  0.2236068\n",
      "  0.         0.         0.2236068  0.         0.2236068  0.2236068\n",
      "  0.2236068  0.         0.         0.2236068  0.         0.2236068\n",
      "  0.         0.         0.2236068  0.         0.         0.2236068\n",
      "  0.2236068  0.         0.         0.        ]\n",
      " [0.28867513 0.         0.28867513 0.         0.28867513 0.\n",
      "  0.28867513 0.         0.         0.         0.28867513 0.\n",
      "  0.         0.         0.         0.28867513 0.         0.28867513\n",
      "  0.         0.         0.         0.28867513 0.         0.\n",
      "  0.         0.28867513 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.28867513 0.         0.         0.28867513 0.         0.\n",
      "  0.         0.         0.28867513 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.37796447 0.         0.         0.         0.         0.\n",
      "  0.         0.37796447 0.         0.         0.         0.\n",
      "  0.37796447 0.         0.         0.         0.         0.\n",
      "  0.         0.37796447 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.37796447 0.\n",
      "  0.         0.37796447 0.         0.37796447]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.37796447 0.         0.         0.\n",
      "  0.         0.         0.37796447 0.         0.         0.\n",
      "  0.37796447 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.37796447 0.         0.\n",
      "  0.         0.         0.37796447 0.         0.37796447 0.\n",
      "  0.         0.37796447 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "-------------------------------------------------\n",
      "FEATURES\n",
      "['able to match' 'and down us' 'and intuition of'\n",
      " 'autonomous vehicles could' 'be able to' 'city streets and'\n",
      " 'computer ever be' 'could someday shuttle' 'does society need'\n",
      " 'down us highways' 'ever be able' 'highways raises some'\n",
      " 'how streets are' 'idea that masses' 'in the first' 'intuition of human'\n",
      " 'masses of autonomous' 'match the safety' 'need this technology'\n",
      " 'need to rethink' 'of autonomous vehicles' 'of human driver'\n",
      " 'passengers through city' 'raises some big' 'rethink how streets'\n",
      " 'safety and intuition' 'shuttle passengers through' 'society need this'\n",
      " 'some big questions' 'someday shuttle passengers' 'streets and down'\n",
      " 'streets are designed' 'technology in the' 'that masses of'\n",
      " 'the first place' 'the idea that' 'the safety and' 'this technology in'\n",
      " 'through city streets' 'to match the' 'to rethink how'\n",
      " 'us highways raises' 'vehicles could someday' 'we need to'\n",
      " 'will computer ever' 'will we need']\n",
      "-------------------------------------------------\n",
      "VOCABULARY\n",
      "{'the idea that': 35, 'idea that masses': 13, 'that masses of': 33, 'masses of autonomous': 16, 'of autonomous vehicles': 20, 'autonomous vehicles could': 3, 'vehicles could someday': 42, 'could someday shuttle': 7, 'someday shuttle passengers': 29, 'shuttle passengers through': 26, 'passengers through city': 22, 'through city streets': 38, 'city streets and': 5, 'streets and down': 30, 'and down us': 1, 'down us highways': 9, 'us highways raises': 41, 'highways raises some': 11, 'raises some big': 23, 'some big questions': 28, 'will computer ever': 44, 'computer ever be': 6, 'ever be able': 10, 'be able to': 4, 'able to match': 0, 'to match the': 39, 'match the safety': 17, 'the safety and': 36, 'safety and intuition': 25, 'and intuition of': 2, 'intuition of human': 15, 'of human driver': 21, 'will we need': 45, 'we need to': 43, 'need to rethink': 19, 'to rethink how': 40, 'rethink how streets': 24, 'how streets are': 12, 'streets are designed': 31, 'does society need': 8, 'society need this': 27, 'need this technology': 18, 'this technology in': 37, 'technology in the': 32, 'in the first': 14, 'the first place': 34}\n"
     ]
    }
   ],
   "source": [
    "def tfidf_3gram(corpus):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(3,3))\n",
    "    matrix = vectorizer.fit_transform(corpus)\n",
    "    print('BAG OF WORDS')\n",
    "    print(matrix.toarray())\n",
    "    print('-------------------------------------------------')\n",
    "    print('FEATURES')\n",
    "    print(vectorizer.get_feature_names_out())\n",
    "    print('-------------------------------------------------')\n",
    "    print('VOCABULARY')\n",
    "    print(vectorizer.vocabulary_)\n",
    "    \n",
    "tfidf_3gram(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22a6789-d407-49cb-b664-9568cc5e4fab",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9550430-f8ca-4830-9957-840d5541c620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "481deab7-433e-4666-847d-ba44e8b29e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ''\n",
    "with open('sad.thorn','r',encoding='utf-8-sig') as file:\n",
    "    data = file.read()\n",
    "    data = data.replace('þ','|')\n",
    "\n",
    "with open('input.csv', 'w') as csvFile:\n",
    "    csvFile.writelines(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5700dcd0-6904-4523-a35e-7eee0fa063db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('input.csv', sep = '|', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ecdcbad-dc40-4359-a551-fc11e354f7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576695</th>\n",
       "      <td>1578623</td>\n",
       "      <td>1</td>\n",
       "      <td>Zzzzzz.... Finally! Night tweeters!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576696</th>\n",
       "      <td>1578624</td>\n",
       "      <td>1</td>\n",
       "      <td>Zzzzzzz, sleep well people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576697</th>\n",
       "      <td>1578625</td>\n",
       "      <td>0</td>\n",
       "      <td>ZzzZzZzzzZ... wait no I have homework.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576698</th>\n",
       "      <td>1578626</td>\n",
       "      <td>0</td>\n",
       "      <td>ZzZzzzZZZZzzz meh, what am I doing up again?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576699</th>\n",
       "      <td>1578627</td>\n",
       "      <td>0</td>\n",
       "      <td>Zzzzzzzzzzzzzzzzzzz, I wish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1576700 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ItemID  Sentiment                                      SentimentText\n",
       "0              1          0                       is so sad for my APL frie...\n",
       "1              2          0                     I missed the New Moon trail...\n",
       "2              3          1                            omg its already 7:30 :O\n",
       "3              4          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4              5          0           i think mi bf is cheating on me!!!   ...\n",
       "...          ...        ...                                                ...\n",
       "1576695  1578623          1               Zzzzzz.... Finally! Night tweeters! \n",
       "1576696  1578624          1                        Zzzzzzz, sleep well people \n",
       "1576697  1578625          0            ZzzZzZzzzZ... wait no I have homework. \n",
       "1576698  1578626          0      ZzZzzzZZZZzzz meh, what am I doing up again? \n",
       "1576699  1578627          0                       Zzzzzzzzzzzzzzzzzzz, I wish \n",
       "\n",
       "[1576700 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2970e332-6445-46bd-8460-b0766c208c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "x = vectorizer.fit_transform(df['SentimentText'])\n",
    "y = df['Sentiment']\n",
    "X_train , X_test, y_train, y_test = train_test_split(x, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d11216ad-3630-44e8-b445-26ba31a1cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train) # Add the appropriate test information\n",
    "y_pred = clf.predict(X_test) # Try to predict new tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "978868b7-949f-4f16-bb46-631cfb8ca2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score : 0.7982452877560698\n",
      "Recall Score:  0.7258978326864371\n",
      "F1 - Score 0.7603544792644688\n"
     ]
    }
   ],
   "source": [
    "print('Precision Score :', precision_score(y_test,y_pred))\n",
    "print('Recall Score: ', recall_score(y_test,y_pred))\n",
    "print('F1 - Score',f1_score(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
